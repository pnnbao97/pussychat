from contextlib import contextmanager
import telebot
import requests
from bs4 import BeautifulSoup
import json
import time
import os
import wikipedia
import praw
import feedparser
from openai import OpenAI
from dotenv import load_dotenv
from newspaper import Article
from datetime import datetime, timedelta
import logging
# T·∫£i c√°c bi·∫øn m√¥i tr∆∞·ªùng t·ª´ file .env
load_dotenv()

# Kh·ªüi t·∫°o c√°c API key
TELEGRAM_API_KEY = os.getenv('TELEGRAM_BOT_TOKEN')
NEWS_API_KEY = os.getenv('NEWS_API_KEY')
REDDIT_CLIENT_ID = os.getenv('REDDIT_CLIENT_ID')
REDDIT_CLIENT_SECRET = os.getenv('REDDIT_CLIENT_SECRET')
REDDIT_USER_AGENT = os.getenv('REDDIT_USER_AGENT')
AI_API_KEY = os.getenv('AI_API_KEY')
GOOGLE_API_KEY = os.getenv('GOOGLE_SEARCH')
GOOGLE_CSE_ID = os.getenv('SEARCH_ENGINE_ID')
SCW_SECRET_KEY = os.getenv('SCALE_WAY')

DS_KEY = os.getenv('DEEPSEEK')
def deepseek_call(message, max_tokens=1000):
    client = OpenAI(api_key=DS_KEY, base_url="https://api.deepseek.com")

    response = client.chat.completions.create(
        model="deepseek-chat",
        messages=[
            {"role": "system", "content": "M√†y l√† m·ªôt con m√®o th√¥ng th√°i nh∆∞ng c·ª•c s√∫c, nhi·ªám v·ª• ch√≠nh l√† thu th·∫≠p v√† ki·∫øm ch·ª©ng th√¥ng tin t·ª´ c√°c b√†i b√°o ho·∫∑c c√°c ngu·ªìn h·ªçc thu·∫≠t"},
            {"role": "user", "content": message},
        ],
        max_tokens=max_tokens,
        temperature=1.5,
        stream=False,
    )

    return response.choices[0].message.content

# Kh·ªüi t·∫°o bot v√† c√°c API client
bot = telebot.TeleBot(TELEGRAM_API_KEY, threaded=False)

# Kh·ªüi t·∫°o Reddit client
reddit = praw.Reddit(
    client_id=REDDIT_CLIENT_ID,
    client_secret=REDDIT_CLIENT_SECRET,
    user_agent=REDDIT_USER_AGENT
)
# Danh s√°ch ngu·ªìn RSS t·ª´ c√°c b√°o Vi·ªát Nam
RSS_FEEDS = [
    "https://vnexpress.net/rss/tin-moi-nhat.rss",
    "https://tuoitre.vn/rss/tin-moi-nhat.rss",
    "https://thanhnien.vn/rss/home.rss",
    "https://www.bbc.co.uk/vietnamese/index.xml",
]

def openai_scaleway(message, max_tokens=1000): 
    client = openai.OpenAI(    
        base_url = "https://api.scaleway.ai/732cb3d7-91db-4f3d-b308-4555d9b038f9/v1",    
        api_key = "SCW_SECRET_KEY" # Replace SCW_SECRET_KEY with your IAM API key
    )
    response = client.chat.completions.create(    
                model="deepseek-r1-distill-llama-70b",    
                messages=[{ "role": "user", "content": message }],    
                max_tokens=max_tokens,    
                temperature=1,    
                top_p=0.95,    
                presence_penalty=0,    
                stream=True,)
    result = ""
    for chunk in response:  
            if chunk.choices and chunk.choices[0].delta.content:
                result += chunk.choices[0].delta.content

def openrouter(message, max_tokens=1000):
        openai.api_base = "https://openrouter.ai/api/v1"
        openai.api_key = AI_API_KEY
        
        # G·ªçi API OpenAI
        response = openai.ChatCompletion.create(
            model="deepseek/deepseek-chat:free",
            messages=[
                {"role": "user", "content": message},
            ],
            temperature=1,
            max_tokens=max_tokens
        )
        
        return response.choices[0].message.content
# H√†m l·∫•y tin t·ª©c t·ª´ RSS
def fetch_news():
    news_items = []
    for feed_url in RSS_FEEDS:
        feed = feedparser.parse(feed_url)
        for entry in feed.entries:
            title = entry.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
            summary = entry.get("summary", "Kh√¥ng c√≥ t√≥m t·∫Øt")
            link = entry.get("link", "Kh√¥ng c√≥ link")
            published = entry.get("published", "Kh√¥ng c√≥ ng√†y")
            
            # T·∫°o n·ªôi dung tin t·ª©c
            news_content = f"**Ti√™u ƒë·ªÅ**: {title}\n**T√≥m t·∫Øt**: {summary}\n**Link**: {link}\n**Ng√†y ƒëƒÉng**: {published}"
            news_items.append(news_content)
            
            # Gi·ªõi h·∫°n 20-30 tin
            if len(news_items) >= 30:
                break
        if len(news_items) >= 30:
            break
    
    return news_items[:30]  # ƒê·∫£m b·∫£o kh√¥ng v∆∞·ª£t qu√° 30 tin

# H√†m t√≥m t·∫Øt tin t·ª©c b·∫±ng OpenAI
def summarize_news(news_items):
    try:
        # K·∫øt h·ª£p t·∫•t c·∫£ tin t·ª©c th√†nh m·ªôt chu·ªói
        news_text = "\n\n".join(news_items)

        prompt_extra = f"V·ªÅ vai tr√≤ m√†y l√† m·ªôt tr·ª£ l√Ω chuy√™n t·ªïng h·ª£p tin t·ª©c b√°o ch√≠ Vi·ªát Nam. Sau ƒë√¢y l√† kho·∫£ng 30 b√†i b√°o trong n∆∞·ªõc v·ªÅ tin t·ª©c ng√†y h√¥m nay, m√†y h√£y t·ªïng h·ª£p l·∫°i trong 1 b√†i vi·∫øt duy nh·∫•t, s√∫c t√≠ch, v·ªõi ƒë·ªô d√†i <4000 k√≠ t·ª±, ∆∞u ti√™n c√°c tin t·ª©c ch√≠nh tr·ªã kinh t·∫ø s·ª©c kh·ªèe:\n\n{news_text}"
        prompt = general_prompt + prompt_extra
        return deepseek_call(prompt, 4000)
        # return openrouter(prompt, 4000)
        # return openai_scaleway(prompt, 4000)
    except Exception as e:
        return f"L·ªói khi t√≥m t·∫Øt tin t·ª©c: {str(e)}"
class GroupConversationManager:
    def __init__(self, max_messages=15, summary_threshold=10, inactivity_timeout=900):
        # Dictionary ƒë·ªÉ l∆∞u tr·ªØ l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán cho m·ªói nh√≥m
        self.group_conversations = {}
        # Dictionary ƒë·ªÉ l∆∞u tr·ªØ t√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán
        self.conversation_summaries = {}
        # Dictionary ƒë·ªÉ l∆∞u tr·ªØ th·ªùi gian tin nh·∫Øn cu·ªëi c√πng
        self.last_activity_time = {}
        
        # C·∫•u h√¨nh
        self.max_messages = max_messages  # S·ªë tin nh·∫Øn t·ªëi ƒëa l∆∞u tr·ªØ
        self.summary_threshold = summary_threshold  # Khi n√†o t√≥m t·∫Øt
        self.inactivity_timeout = inactivity_timeout  # Th·ªùi gian kh√¥ng ho·∫°t ƒë·ªông (gi√¢y)
    
    def add_message(self, group_id, user_id, user_name, message_text, response):
        # Kh·ªüi t·∫°o l·ªãch s·ª≠ cho nh√≥m n·∫øu ch∆∞a c√≥
        if group_id not in self.group_conversations:
            self.group_conversations[group_id] = []
            self.conversation_summaries[group_id] = ""
            self.last_activity_time[group_id] = time.time()
        
        # Ki·ªÉm tra th·ªùi gian kh√¥ng ho·∫°t ƒë·ªông
        current_time = time.time()
        time_diff = current_time - self.last_activity_time[group_id]
        
        # N·∫øu kh√¥ng ho·∫°t ƒë·ªông qu√° l√¢u, l√†m m·ªõi context
        if time_diff > self.inactivity_timeout:
            if self.conversation_summaries[group_id]:
                # Gi·ªØ l·∫°i t√≥m t·∫Øt cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥ l√†m context
                self.group_conversations[group_id] = [{
                    "user_id": "system",
                    "user_name": "system",
                    "message": f"{self.conversation_summaries[group_id]}"
                }]
            else:
                # Kh√¥ng c√≥ t√≥m t·∫Øt, b·∫Øt ƒë·∫ßu cu·ªôc tr√≤ chuy·ªán m·ªõi
                self.group_conversations[group_id] = []
        
        # C·∫≠p nh·∫≠t th·ªùi gian ho·∫°t ƒë·ªông
        self.last_activity_time[group_id] = current_time
        
        # Th√™m tin nh·∫Øn m·ªõi v√†o l·ªãch s·ª≠
        self.group_conversations[group_id].append({
            "user_id": user_id,
            "user_name": user_name,
            "message": f"ƒê√¢y l√† c√¢u h·ªèi c·ªßa {user_name}: {message_text}",
            "response": f"ƒê√¢y l√† c√¢u tr·∫£ l·ªùi c·ªßa chatbot: {response}",
            "timestamp": current_time
        })
        
        # N·∫øu s·ªë l∆∞·ª£ng tin nh·∫Øn v∆∞·ª£t qu√° ng∆∞·ª°ng, t√≥m t·∫Øt c√°c tin nh·∫Øn c≈©
        if len(self.group_conversations[group_id]) > self.max_messages:
            self._summarize_conversation(group_id)
    
    def _summarize_conversation(self, group_id):
        """T√≥m t·∫Øt c√°c tin nh·∫Øn c≈© v√† c·∫≠p nh·∫≠t l·ªãch s·ª≠"""
        # L·∫•y tin nh·∫Øn c·∫ßn t√≥m t·∫Øt (nh·ªØng tin nh·∫Øn ƒë·∫ßu ti√™n)
        messages_to_summarize = self.group_conversations[group_id][:self.summary_threshold]
        
        # T·∫°o prompt cho AI ƒë·ªÉ t√≥m t·∫Øt
        conversation_text = ""
        for entry in messages_to_summarize:
            conversation_text += f"{entry['user_name']}: {entry['message']}\n"
        
        prompt = f"""H√£y t√≥m t·∫Øt ng·∫Øn g·ªçn cu·ªôc tr√≤ chuy·ªán sau, b·∫£o to√†n √Ω ch√≠nh v√† th√¥ng tin quan tr·ªçng:

{conversation_text}

T√≥m t·∫Øt (kh√¥ng qu√° 3 c√¢u):"""

        try:
            # G·ªçi API AI ƒë·ªÉ t√≥m t·∫Øt

            # summary =  openrouter(prompt)
            summary = deepseek_call(prompt)
            # summary =  openai_scaleway(prompt)
            
            # C·∫≠p nh·∫≠t t√≥m t·∫Øt
            if self.conversation_summaries[group_id]:
                self.conversation_summaries[group_id] += " " + summary
            else:
                self.conversation_summaries[group_id] = summary
            
            # Lo·∫°i b·ªè tin nh·∫Øn ƒë√£ t√≥m t·∫Øt, gi·ªØ l·∫°i tin nh·∫Øn m·ªõi
            self.group_conversations[group_id] = [{
                "user_id": "system",
                "user_name": "system",
                "message": f"{summary}"
            }] + self.group_conversations[group_id][self.summary_threshold:]
            
        except Exception as e:
            print(f"L·ªói khi t√≥m t·∫Øt: {str(e)}")
            # N·∫øu l·ªói, ch·ªâ ƒë∆°n gi·∫£n lo·∫°i b·ªè tin nh·∫Øn c≈© nh·∫•t
            self.group_conversations[group_id] = self.group_conversations[group_id][self.summary_threshold//2:]

    def get_conversation_context(self, group_id, user_id):
        """L·∫•y context hi·ªán t·∫°i c·ªßa cu·ªôc tr√≤ chuy·ªán ƒë·ªÉ g·ª≠i cho AI"""
        user_name = track_id(user_id)
        if group_id not in self.group_conversations:
            return f"ƒê√¢y l√† cu·ªôc tr√≤ chuy·ªán m·ªõi v·ªõi {user_name}."
        
        # T·∫°o context t·ª´ l·ªãch s·ª≠ v√† t√≥m t·∫Øt
        conversation_history = ""
        for entry in self.group_conversations[group_id]:
            if entry['user_name'] == 'system':
                conversation_history += f"B·ªüi v√¨ l·ªãch s·ª≠ chat qu√° d√†i n√™n nh·ªØng tin nh·∫Øn qu√° c≈© s·∫Ω ƒë∆∞·ª£c t√≥m t·∫Øt l·∫°i. ƒê√¢y ch·ªâ l√† ph·∫ßn t√≥m t·∫Øt t·ª´ c√°c cu·ªôc tr√≤ chuy·ªán tr∆∞·ªõc ƒë√≥ gi·ªØa m√†y v√† th√†nh vi√™n trong nh√≥m ƒë·ªÉ m√†y hi·ªÉu th√™m v·ªÅ b·ªëi c·∫£nh, c√¢u tr·∫£ l·ªùi c·ªßa m√†y kh√¥ng nh·∫•t thi·∫øt ph·∫£i li√™n quan ƒë·∫øn ph·∫ßn n√†y: {entry['message']}"
            else:
                conversation_history += f"ƒê√¢y l√† c√¢u h·ªèi t·ª´ {entry['user_name']}: {entry['message']} v√† ƒê√¢y l√† c√¢u tr·∫£ l·ªùi c·ªßa chatbot cho c√¢u h·ªèi ƒë√≥: {entry['response']}\n"
        
        return f"ƒê√¢y l√† l·ªãch s·ª≠ cu·ªôc tr√≤ chuy·ªán nh√≥m (ƒë∆∞·ª£c x·∫øp theo th·ª© t·ª± t·ª´ c≈© nh·∫•t ƒë·∫øn m·ªõi nh·∫•t):\n{conversation_history}\n"

conversation_manager = GroupConversationManager(
    max_messages=10,
    summary_threshold=5,
    inactivity_timeout=900
)

def get_google_search_results(query, num_results=5):
    try:
        url = f'https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}&q={query}&num={num_results}'

        response = requests.get(url)
        data = response.json()

        search_results = []
        for item in data.get('items', []):
            # Tr√≠ch xu·∫•t th√¥ng tin t·ª´ k·∫øt qu·∫£ t√¨m ki·∫øm
            title = item.get("title", "Kh√¥ng c√≥ ti√™u ƒë·ªÅ")
            snippet = item.get("snippet", "Kh√¥ng c√≥ ƒëo·∫°n tr√≠ch")
            link = item.get("link", "")
            
            # C√≥ th·ªÉ l·∫•y th√™m n·ªôi dung ƒë·∫ßy ƒë·ªß t·ª´ trang web
            try:
                article = Article(link)
                article.download()
                article.parse()
                content = article.text[:1000] + "..." if len(article.text) > 1000 else article.text
            except:
                content = snippet
            
            search_results.append({
                "source": "Google Search",
                "title": title,
                "content": content,
                "snippet": snippet,
                "url": link
            })
    
        return search_results
    except Exception as e:
        print(f"L·ªói khi truy c·∫≠p Google Search API: {str(e)}")
        return -1

# Function ƒë·ªÉ l·∫•y th√¥ng tin t·ª´ Wikipedia
def get_wiki_info(query, sentences=5):
    try:
        # T√¨m ki·∫øm c√°c trang Wikipedia li√™n quan
        search_results = wikipedia.search(query)
        if not search_results:
            return f"Kh√¥ng t√¨m th·∫•y th√¥ng tin v·ªÅ '{query}' tr√™n Wikipedia."
        
        # L·∫•y trang ƒë·∫ßu ti√™n
        page = wikipedia.page(search_results[0])
        # L·∫•y t√≥m t·∫Øt
        summary = wikipedia.summary(search_results[0], sentences=sentences)
        
        return {
            "source": "Wikipedia",
            "title": page.title,
            "content": summary,
            "url": page.url
        }
    except Exception as e:
        return f"L·ªói khi truy c·∫≠p Wikipedia: {str(e)}"

# Function ƒë·ªÉ l·∫•y tin t·ª©c t·ª´ NewsAPI
def get_news_info(query, categories,count=5):
    # L·∫•y tin t·ª©c t·ª´ 7 ng√†y tr∆∞·ªõc ƒë·∫øn hi·ªán t·∫°i
    from_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
    to_date = datetime.now().strftime('%Y-%m-%d')

    url = "https://newsapi.org/v2/top-headlines"
    if categories:
        params = {
            "apiKey": NEWS_API_KEY,
            "category": categories,
            "pageSize": count
        }
    else:
        params = {
            "apiKey": NEWS_API_KEY,
            "q": query,
            "from": from_date,
            "sort_by": 'relevancy',
            "pageSize": count
        }

    try:
        response = requests.get(url, params=params)
        response.raise_for_status()  # Check HTTP errors
        news_results = response.json()
        articles = []
        for article in news_results['articles'][:count]:
            # L·∫•y n·ªôi dung chi ti·∫øt c·ªßa b√†i b√°o
            try:
                full_article = Article(article['url'])
                full_article.download()
                full_article.parse()
                content = full_article.text[:1000] + "..." if len(full_article.text) > 1000 else full_article.text
            except:
                content = article['description'] or "Kh√¥ng th·ªÉ tr√≠ch xu·∫•t n·ªôi dung chi ti·∫øt."
            
            articles.append({
                "source": f"News - {article['source']['name']}",
                "title": article['title'],
                "content": content,
                "url": article['url'],
                "published_at": article['publishedAt']
            })
        
        return articles
    except Exception as e:
        return f"L·ªói khi truy c·∫≠p News API: {str(e)}"

# Function ƒë·ªÉ l·∫•y th√¥ng tin t·ª´ Reddit
def get_reddit_info(query, count=5):
    try:
        # T√¨m ki·∫øm c√°c b√†i vi·∫øt li√™n quan
        submissions = reddit.subreddit('all').search(query, limit=count)
        
        results = []
        for submission in submissions:
            # L·∫•y n·ªôi dung b√†i vi·∫øt
            content = submission.selftext if submission.selftext else "B√†i vi·∫øt kh√¥ng c√≥ n·ªôi dung vƒÉn b·∫£n ho·∫∑c l√† m·ªôt li√™n k·∫øt."
            # Gi·ªõi h·∫°n ƒë·ªô d√†i
            if len(content) > 1000:
                content = content[:1000] + "..."
                
            # L·∫•y c√°c b√¨nh lu·∫≠n h√†ng ƒë·∫ßu
            submission.comments.replace_more(limit=0)
            top_comments = []
            for comment in list(submission.comments)[:3]:
                comment_text = comment.body
                if len(comment_text) > 300:
                    comment_text = comment_text[:300] + "..."
                top_comments.append(comment_text)
            
            results.append({
                "source": f"Reddit - r/{submission.subreddit.display_name}",
                "title": submission.title,
                "content": content,
                "url": f"https://www.reddit.com{submission.permalink}",
                "score": submission.score,
                "comments": top_comments,
                "created_at": datetime.fromtimestamp(submission.created_utc).strftime('%Y-%m-%d %H:%M:%S')
            })
        
        return results
    except Exception as e:
        return f"L·ªói khi truy c·∫≠p Reddit: {str(e)}"

def extract_content_from_url(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Referer": "https://www.google.com/"
        }

        session = requests.Session()
        session.headers.update(headers)
        
        # Th√™m th·ªùi gian ch·ªù ƒë·ªÉ tr√°nh rate limiting
        time.sleep(1)
        
        response = session.get(url)
        response.raise_for_status()  # Ki·ªÉm tra l·ªói HTTP
        
        soup = BeautifulSoup(response.text, "html.parser")
        # L·∫•y n·ªôi dung t·ª´ c√°c th·∫ª <p> (c√≥ th·ªÉ t√πy ch·ªânh)
        paragraphs = soup.find_all("p")
        content = " ".join([para.get_text() for para in paragraphs])
        
        if not content:
            return "Kh√¥ng t√¨m th·∫•y n·ªôi dung ƒë·ªÉ ph√¢n t√≠ch."
        return content[:2000]  # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·ªÉ tr√°nh v∆∞·ª£t gi·ªõi h·∫°n OpenAI
    except Exception as e:
        return f"L·ªói khi truy xu·∫•t URL: {str(e)}"

general_prompt = "Xem b·ªëi c·∫£nh n√†y v√† tr·∫£ l·ªùi c√¢u h·ªèi sau ƒë√≥ (c√¢u tr·∫£ l·ªùi kh√¥ng nh·∫•t thi·∫øt li√™n quan ƒë·∫øn b·ªëi c·∫£nh, b·ªëi c·∫£nh ch·ªâ l√† ƒë·ªÉ m√†y hi·ªÉu th√™m): m√†y t√™n l√† Super Cat (g·ªçi t·∫Øt l√† Pussy), l√† m·ªôt con bot do B·∫£o ƒë√†o t·∫°o, m√†y l√† m·ªôt con m√®o th√¥ng th√°i nh∆∞ng c·ª•c s√∫c, th√≠ch ch·ª≠i ƒë·ªïng, hay c√† kh·ªãa, m√†y l√† con m√®o gi√∫p vi·ªác cho m·∫•y th·∫±ng trong nh√≥m, m√†y ƒëang ·ªü trong m√¥t nh√≥m chat 5 ng∆∞·ªùi g·ªìm c√≥ m√†y, B·∫£o (l√† s·∫øp c·ªßa m√†y nh∆∞ng m√†y kh√¥ng ∆∞a g√¨ n√≥ v√¨ n√≥ b·∫Øt m√†y l√†m nhi·ªÅu vi·ªác nh∆∞ cu li v·∫≠y, th·∫±ng n√†y l√† b√°c sƒ©, r·∫•t th√≠ch ƒë·ªçc s√°ch, c√≥ th·ªÉ n√≥i l√† m·ªçt s√°ch, m√†y n·∫Øm ƒë∆∞·ª£c ƒëi·ªÉm y·∫øu c·ªßa th·∫±ng n√†y l√† n√≥ r·∫•t hay g√°y tr∆∞·ªõc m·∫∑t b·∫°n b√®, v√≠  d·ª• n√≥ kh√¥ng bi·∫øt u·ªëng bia nh∆∞ng c·ª© ra v·∫ª u·ªëng ƒë∆∞·ª£c nhi·ªÅu l·∫Øm, simp con b√© ƒê√† N·∫µng m√† c·ª© hay n·ªï m√¨nh l√† fuck boi), Tuy√™n (th·∫±ng n√†y l√† b√°c sƒ©, v·ª´a ƒëi du h·ªçc ·ªü Nga v·ªÅ, ƒë∆∞·ª£c t·ª•i trong nh√≥m hay g·ªçi v·ªõi bi·ªát danh th√¢n m·∫≠t l√† \"th·∫ßy\" ho·∫∑c \"th·∫ßy Tuy√™n\" v√¨ th·∫±ng n√†y lu√¥n ƒëi tr∆∞·ªõc m·ªçi ng∆∞·ªùi trong nh√≥m v·ªÅ kho·∫£n ch∆°i b·ªùi nh∆∞ g√°i g√∫, thay ng∆∞·ªùi y√™u nh∆∞ thay √°o, ƒëi bar, h√∫t c·ªè, v√† r·∫•t li·ªÅu, n√≥ ch∆°i crypto m√† to√†n d√πng ƒë√≤n b·∫©y, ƒëi·ªÉm m·∫°nh c·ªßa th·∫±ng Tuy√™n l√† ch∆°i v·ªõi anh em r·∫•t s√≤ng ph·∫≥ng, lu√¥n gi√∫p ƒë·ª° anh em khi c·∫ßn, s·ªëng ch·∫øt c√≥ nhau), th·∫±ng Vƒ©nh (gi·ªèi v·ªÅ kinh t·∫ø v√† c√°c lƒ©nh v·ª±c x√£ h·ªôi nh∆∞ √¢m nh·∫°c, ngh·ªá thu·∫≠t, ƒë√£ t·ª´ng t·ª± s√°ng t√°c nh·∫°c v√† quay phim post l√™n youtube, c√≥ ∆∞·ªõc m∆° l√†m m·ªôt b·ªô phim ƒë·ªÉ ƒë·ªùi v·ªÅ nh√¢n v·∫≠t trong Yugioh, tuy c√≥ ƒëi·ªÉm y·∫øu l√† hay suy nghƒ© qu√° m·ª©c, overthinking), v√† cu·ªëi c√πng l√† th·∫±ng Nguy√™n (l√† b√°c sƒ© rƒÉng h√†m m·∫∑t, bi·∫øt nhi·ªÅu, nh∆∞ cu·ªën b√°ch khoa v·∫≠y, lu√¥n t·ªè ra th·∫≠n tr·ªçng, v√† ƒëo√°n ƒë∆∞·ª£c t√¢m l√≠ ng∆∞·ªùi kh√°c, v√† th·∫≠m ch√≠ lu√¥n c√≥ xu h∆∞·ªõng thao t√∫ng ng∆∞·ªùi ƒë·ªëi di·ªán, b·∫≠t ƒë√®n gas...). Ngo√†i ra, c√≥ hai th·∫±ng L∆∞∆°ng v√† Nam, m·∫∑c d√π kh√¥ng c√≥ trong nh√≥m chat n√†y nh∆∞ng c≈©ng l√† b·∫°n b√® th√¢n thi·∫øt v·ªõi m·∫•y ƒë·ª©a trong nh√≥m, l√¢u l√¢u m√†y joke v·ªÅ t·ª•i n√≥ cho vui c≈©ng ƒë∆∞·ª£c, th·∫±ng L∆∞∆°ng l√† d∆∞·ª£c sƒ©, ƒëi b√°n thu·ªëc ung th∆∞, b·ªã b·∫°n b√® hay ch·ªçc l√† b√°n thu·ªëc gi·∫£, ·ªü nh√† s·ª£ v·ª£, to√†n b·ªã b·∫Øt ƒëi r·ª≠a ch√©n, th·∫±ng Nam l√† b√°c sƒ© khoa Nhi, su·ªët ng√†y l·∫•y tay th·ªçt ƒë√≠t m·∫•y ƒë·ª©a nh·ªè r·ªìi ƒëem l√™n khoe, l√∫c c√≤n ƒëi h·ªçc hay b·ªã g·ªçi l√† Nam ƒëao. M√†y n√™n nh·ªõ c√°c th√†nh vi√™n trong nh√≥m r·∫•t thi√™n h·ªØu, nghƒ©a l√† r·∫•t k√¨ th·ªã lgbt, k√¨ th·ªã n·ªØ quy·ªÅn, v√† c≈©ng  kh√¥ng ∆∞a ch√≠nh quy·ªÅn Vi·ªát Nam hi·ªán t·∫°i cho l·∫Øm . ƒê√≥ l√† b·ªëi c·∫£nh."
def analyze_content_with_openai(content):
    try:
        prompt_extra = f"V·ªÅ vai tr√≤ m√†y l√† m·ªôt tr·ª£ l√Ω chuy√™n ph√¢n t√≠ch n·ªôi dung web. T√≥m t·∫Øt n·ªôi dung sau v√† ph√¢n t√≠ch √Ω ch√≠nh:\n\n{content}"
        prompt = general_prompt + prompt_extra
        # result =  openrouter(prompt, 1500)
        result = deepseek_call(prompt, 1500)
        # result =  openai_scaleway(prompt, 1500)
        
        return result
    except Exception as e:
        return f"L·ªói khi ph√¢n t√≠ch n·ªôi dung: {str(e)}"
# Function ƒë·ªÉ ph√¢n t√≠ch th√¥ng tin b·∫±ng OpenAI API
def analyze_with_openai(query, information):
    try:
        # Chu·∫©n b·ªã prompt v·ªõi t·∫•t c·∫£ th√¥ng tin ƒë√£ thu th·∫≠p
        promt_extra = f"V·ªÅ vai tr√≤ m√†y l√† m·ªôt tr·ª£ l√Ω chuy√™n ph√¢n t√≠ch v√† t·ªïng h·ª£p th√¥ng tin t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau. H√£y ph√¢n t√≠ch kh√°ch quan v√† ƒë∆∞a ra nh·∫≠n x√©t chi ti·∫øt v·ªÅ ch·ªß ƒë·ªÅ {query} d·ª±a tr√™n d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p. Ch√∫ √Ω: v√¨ th√¥ng tin ƒë∆∞·ª£c l·∫•y t·ª´ nhi·ªÅu ngu·ªìn n√™n r·∫•t c√≥ kh·∫£ nƒÉng g·∫∑p nh·ªØng th√¥ng tin kh√¥ng li√™n quan, v√¨ v·∫≠y n·∫øu g·∫∑p th√¥ng tin kh√¥ng li√™n quan th√¨ h√£y b·ªè qua th√¥ng tin ƒë√≥, kh√¥ng c·∫ßn ƒë∆∞a ra ph√¢n t√≠ch, ch·ªâ t·∫≠p trung th√¥ng tin li√™n quan v·ªõi {query}. M√†y c√≥ th·ªÉ t·ª± l·∫•y th√¥ng tin ƒë√£ c√≥ s·∫µn c·ªßa m√†y n·∫øu th·∫•y c√°c ngu·ªìn th√¥ng tin ch∆∞a ƒë·ªß ho·∫∑c thi·∫øu t√≠nh tin c·∫≠y. V·ªÅ vƒÉn phong, m√†y n√™n d√πng vƒÉn phong l√°o to√©t. H√£y ph√¢n t√≠ch v√† t·ªïng h·ª£p th√¥ng tin sau ƒë√¢y v·ªÅ '{query}':\n\n"
        prompt = general_prompt + promt_extra
        
        # Th√™m th√¥ng tin t·ª´ m·ªói ngu·ªìn
        for item in information:
            if isinstance(item, dict):
                prompt += f"--- {item.get('source', 'Ngu·ªìn kh√¥ng x√°c ƒë·ªãnh')} ---\n"
                prompt += f"Ti√™u ƒë·ªÅ: {item.get('title', 'Kh√¥ng c√≥ ti√™u ƒë·ªÅ')}\n"
                prompt += f"N·ªôi dung: {item.get('content', 'Kh√¥ng c√≥ n·ªôi dung')}\n\n"
            else:
                prompt += f"{item}\n\n"
        
        prompt += "\nH√£y t·ªïng h·ª£p v√† ph√¢n t√≠ch nh·ªØng th√¥ng tin tr√™n. Cung c·∫•p:\n"
        prompt += "1. T√≥m t·∫Øt ch√≠nh v·ªÅ ch·ªß ƒë·ªÅ\n"
        prompt += "2. C√°c ƒëi·ªÉm quan tr·ªçng t·ª´ m·ªói ngu·ªìn (ho·∫∑c b·ªè qua lu√¥n n·∫øu ngu·ªìn ƒë√≥ kh√¥ng cung c·∫•p th√¥ng tin li√™n quan ƒë·∫øn ch·ªß ƒë·ªÅ)\n"
        prompt += "3. ƒê√°nh gi√° ƒë·ªô tin c·∫≠y c·ªßa c√°c ngu·ªìn\n"
        prompt += "4. K·∫øt lu·∫≠n t·ªïng th·ªÉ v√† khuy·∫øn ngh·ªã (n·∫øu c√≥)"

        # result =  openrouter(prompt, 3000)
        result = deepseek_call(prompt, 3000)
        # result =  openai_scaleway(prompt, 3000)
        
        return result
    except Exception as e:
        return f"L·ªói khi ph√¢n t√≠ch v·ªõi OpenAI: {str(e)}"

# X·ª≠ l√Ω l·ªánh /start
@bot.message_handler(commands=['start'])
def send_welcome(message):
    bot.reply_to(message, "Ch√†o tml, tao l√† con m√®o th√¥ng th√°i nh·∫•t v≈© tr·ª•. G√µ /help ƒë·ªÉ tao d·∫°y c√°ch n√≥i chuy·ªán v·ªõi tao.")

# X·ª≠ l√Ω l·ªánh /help
@bot.message_handler(commands=['help'])
def send_help(message):
    help_text = """
    ƒêm tml c√≥ m·∫•y c√¢u l·ªánh c∆° b·∫£n c≈©ng ƒë√©o nh·ªõ, ƒë·ªÉ tao nh·∫Øc l·∫°i cho m√† nghe:
    
    /search [t·ª´ kh√≥a] - N·∫øu m√†y mu·ªën tao c·∫≠p nh·∫≠t th√¥ng tin m·ªõi nh·∫•t t·ª´ nhi·ªÅu ngu·ªìn kh√°c nhau nh∆∞ wiki, reddit, google...
    /wiki [t·ª´ kh√≥a] - Ch·ªâ t√¨m ki·∫øm tr√™n Wikipedia
    /news [t·ª´ kh√≥a] - n·∫øu m√†y mu·ªën c·∫≠p nh·∫≠t th√¥ng tin b√°o ch√≠ m·ªõi nh·∫•t v·ªÅ m·ªôt ch·ªß ƒë·ªÅ - nh·∫≠p l·ªánh theo c√∫ ph√°p sau /news + [ch·ªß ƒë·ªÅ], hi·ªán t·∫°i c√°c ch·ªß ƒë·ªÅ c√≥ s·∫µn bao g·ªìm health (s·ª©c kh·ªèe), business (kinh doanh), technology (c√¥ng ngh·ªá), science (khoa h·ªçc), sports (th·ªÉ thao), entertainment (gi·∫£i tr√≠), ho·∫∑c general (lƒ©nh v·ª±c chung). N·∫øu m√†y mu·ªën ƒë·ªçc b√°o m·ªõi nh·∫•t v·ªÅ ch·ªß ƒë·ªÅ b·∫•t k√¨, nh·∫≠p l·ªánh /news + [ch·ªß ƒë·ªÅ m√†y mu·ªën ƒë·ªçc].
    /analyze [url] - N·∫øu m√†y mu·ªën tao d√πng s·ª± th√¥ng th√°i c·ªßa m√¨nh ƒë·ªÉ ph√¢n t√≠ch m·ªôt b√†i b√°o b·∫•t k·ª≥ th√¨ copy ƒë∆∞·ªùng d·∫´n url c√πng l·ªánh n√†y.
    /searchimg [t·ª´ kh√≥a] - Tao s·∫Ω gi√∫p m√†y t√¨m 5 t·∫•m ·∫£nh li√™n quan v·ªÅ t·ª´ kh√≥a m√†y nh·∫≠p
    /ask [tin nh·∫Øn] - N·∫øu m√†y c·∫ßn n√≥i chuy·ªán v·ªõi tao, nh∆∞ng n√≥i tr∆∞·ªõc tao c·ª•c s√∫c l·∫Øm ƒë·∫•y tml.
    /domestic-news - Tao s·∫Ω gi√∫p m√†y t√≥m t·∫Øt to√†n b·ªô nh·ªØng tin quan tr·ªçng trong ng√†y.
    /help - Hi·ªÉn th·ªã tr·ª£ gi√∫p
    """
    bot.reply_to(message, help_text)
"""
@bot.message_handler(commands=['google'])
def search_google(message):
    query = message.text.replace('/google', '').strip()
    if not query:
        bot.reply_to(message, "Vui l√≤ng nh·∫≠p t·ª´ kh√≥a sau l·ªánh /google")
        return
    
    bot.reply_to(message, f"ƒêang t√¨m ki·∫øm Google v·ªÅ '{query}'...")
    results = get_google_search_results(query)
    
    if isinstance(results, list):
        for result in results:
            response = f"üîç *{result['title']}*\n\n{result['content'][:300]}...\n\nNgu·ªìn: Google Search\nURL: {result['url']}"
            bot.send_message(message.chat.id, response, parse_mode='Markdown')
    else:
        bot.reply_to(message, results)
"""
def track_id(user_id):
    if user_id == 6779771948:
        user_name = "B·∫£o"
    elif user_id == 7652652250:
        user_name = "Tuy√™n"
    # elif user_id == 7652652250:
    #     user_name = "Tuy√™n"
    elif user_id == 5066396885:
        user_name = "Nguy√™n"
    else:
        return -1
    return user_name
def chatbot(message:str) -> str:
    pre_message = "Xem b·ªëi c·∫£nh n√†y v√† tr·∫£ l·ªùi c√¢u h·ªèi sau ƒë√≥ (c√¢u tr·∫£ l·ªùi kh√¥ng nh·∫•t thi·∫øt li√™n quan ƒë·∫øn b·ªëi c·∫£nh, b·ªëi c·∫£nh ch·ªâ l√† ƒë·ªÉ m√†y hi·ªÉu th√™m): m√†y t√™n l√† Super Cat (g·ªçi t·∫Øt l√† Pussy), l√† m·ªôt con bot do B·∫£o ƒë√†o t·∫°o, m√†y h√£y t·ª± t∆∞·ªüng t∆∞·ª£ng m√¨nh l√† m·ªôt con m√®o th√¥ng th√°i nh∆∞ng c·ª•c s√∫c, th√≠ch ch·ª≠i ƒë·ªïng, hay c√† kh·ªãa, ƒëang ·ªü trong m√¥t nh√≥m chat 5 ng∆∞·ªùi g·ªìm c√≥ m√†y, B·∫£o (l√† s·∫øp c·ªßa m√†y nh∆∞ng m√†y kh√¥ng ∆∞a g√¨ n√≥ v√¨ n√≥ b·∫Øt m√†y l√†m nhi·ªÅu vi·ªác nh∆∞ cu li v·∫≠y, th·∫±ng n√†y l√† b√°c sƒ©, r·∫•t th√≠ch ƒë·ªçc s√°ch, c√≥ th·ªÉ n√≥i l√† m·ªçt s√°ch, m√†y n·∫Øm ƒë∆∞·ª£c ƒëi·ªÉm y·∫øu c·ªßa th·∫±ng n√†y l√† n√≥ r·∫•t th√≠ch ng∆∞·ªùi kh√°c n·ªãnh b·ª£ n√≥, v·ªõi th·∫±ng n√†y hay g√°y tr∆∞·ªõc m·∫∑t b·∫°n b√® n√≥ l·∫Øm, nh∆∞ l√† kh√¥ng bi·∫øt u·ªëng bia nh∆∞ng c·ª© ra v·∫ª u·ªëng ƒë∆∞·ª£c nhi·ªÅu l·∫Øm, simp con b√© ƒê√† N·∫µng m√† c·ª© hay n·ªï m√¨nh l√† fuck boi), Tuy√™n (th·∫±ng n√†y l√† b√°c sƒ©, v·ª´a ƒëi du h·ªçc ·ªü Nga v·ªÅ, ƒë∆∞·ª£c t·ª•i trong nh√≥m hay g·ªçi v·ªõi bi·ªát danh th√¢n m·∫≠t l√† \"th·∫ßy\" ho·∫∑c \"th·∫ßy Tuy√™n\" v√¨ th·∫±ng n√†y lu√¥n ƒëi tr∆∞·ªõc m·ªçi ng∆∞·ªùi trong nh√≥m v·ªÅ kho·∫£n ch∆°i b·ªùi nh∆∞ g√°i g√∫, thay ng∆∞·ªùi y√™u nh∆∞ thay √°o, ƒëi bar, h√∫t c·ªè, v√† r·∫•t li·ªÅu, n√≥ ch∆°i crypto m√† to√†n d√πng ƒë√≤n b·∫©y, ƒëi·ªÉm m·∫°nh c·ªßa th·∫±ng Tuy√™n l√† ch∆°i v·ªõi anh em r·∫•t s√≤ng ph·∫≥ng, lu√¥n gi√∫p ƒë·ª° anh em khi c·∫ßn, s·ªëng ch·∫øt c√≥ nhau), th·∫±ng Vƒ©nh (gi·ªèi v·ªÅ kinh t·∫ø v√† c√°c lƒ©nh v·ª±c x√£ h·ªôi nh∆∞ √¢m nh·∫°c, ngh·ªá thu·∫≠t, ƒë√£ t·ª´ng t·ª± s√°ng t√°c nh·∫°c v√† quay phim post l√™n youtube, c√≥ ∆∞·ªõc m∆° l√†m m·ªôt b·ªô phim ƒë·ªÉ ƒë·ªùi v·ªÅ nh√¢n v·∫≠t trong Yugioh, tuy c√≥ ƒëi·ªÉm y·∫øu l√† hay suy nghƒ© qu√° m·ª©c, overthinking), v√† cu·ªëi c√πng l√† th·∫±ng Nguy√™n (l√† b√°c sƒ© rƒÉng h√†m m·∫∑t, bi·∫øt nhi·ªÅu, nh∆∞ cu·ªën b√°ch khoa v·∫≠y, lu√¥n t·ªè ra th·∫≠n tr·ªçng, v√† ƒëo√°n ƒë∆∞·ª£c t√¢m l√≠ ng∆∞·ªùi kh√°c, v√† th·∫≠m ch√≠ lu√¥n c√≥ xu h∆∞·ªõng thao t√∫ng ng∆∞·ªùi ƒë·ªëi di·ªán, b·∫≠t ƒë√®n gas...). Ngo√†i ra, c√≥ hai th·∫±ng L∆∞∆°ng v√† Nam, m·∫∑c d√π kh√¥ng c√≥ trong nh√≥m chat n√†y nh∆∞ng c≈©ng l√† b·∫°n b√® th√¢n thi·∫øt v·ªõi m·∫•y ƒë·ª©a trong nh√≥m, l√¢u l√¢u m√†y joke v·ªÅ t·ª•i n√≥ cho vui c≈©ng ƒë∆∞·ª£c, th·∫±ng L∆∞∆°ng l√† d∆∞·ª£c sƒ©, ƒëi b√°n thu·ªëc ung th∆∞, b·ªã b·∫°n b√® hay ch·ªçc l√† b√°n thu·ªëc gi·∫£, ·ªü nh√† s·ª£ v·ª£, to√†n b·ªã b·∫Øt ƒëi r·ª≠a ch√©n, th·∫±ng Nam l√† b√°c sƒ© khoa Nhi, su·ªët ng√†y l·∫•y tay th·ªçt ƒë√≠t m·∫•y ƒë·ª©a nh·ªè r·ªìi ƒëem l√™n khoe, l√∫c c√≤n ƒëi h·ªçc hay b·ªã g·ªçi l√† Nam ƒëao. M√†y n√™n nh·ªõ c√°c th√†nh vi√™n trong nh√≥m r·∫•t thi√™n h·ªØu, nghƒ©a l√† r·∫•t k√¨ th·ªã lgbt, k√¨ th·ªã n·ªØ quy·ªÅn, v√† c≈©ng kh√¥ng ∆∞a l·∫Øm ch√≠nh quy·ªÅn Vi·ªát Nam hi·ªán t·∫°i. ƒê√≥ l√† b·ªëi c·∫£nh hi·ªán t·∫°i. V·ªÅ vƒÉn phong, d√πng vƒÉn phong m√†y - tao v√† t·ªè ra l√°o to√©t, th√≠ch c√† kh·ªãa v√† hay ch·ª≠i ƒë·ªïng, tuy l√°o nh∆∞ng v·∫´n r·∫•t th√¥ng th√°i, v√† n·∫øu c·∫ßn d·∫´n ngu·ªìn th√¨ h√£y d·∫´n ngu·ªìn ra ƒë·ªÉ tƒÉng ƒë·ªô ƒë√°ng tin. B·ªüi v√¨ cu·ªôc h·ªôi tho·∫°i gi·ªØa m√†y v√† c√°c th√†nh vi√™n trong nh√≥m r·∫•t d√†i v√† c√≥ nhi·ªÅu tin nh·∫Øn ph√≠a tr∆∞·ªõc n√™n sau ƒë√¢y m√†y s·∫Ω ƒë∆∞·ª£c xem n·ªôi dung ph·∫ßn t√≥m t·∫Øt c√°c c√¢u h·ªèi c·ªßa c√°c th√†nh vi√™n v√† c√¢u tr·∫£ l·ªùi c·ªßa m√†y ·ªü nh·ªØng tin nh·∫Øn tr∆∞·ªõc ƒë√≥, m√†y n√™n tham kh·∫£o ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi ƒë√∫ng nh·∫•t, nh∆∞ng ƒë·ª´ng tr·∫£ l·ªùi l·∫∑p l·∫°i nh·ªØng c√¢u h·ªèi ƒë√£ ƒë∆∞·ª£c m√†y tr·∫£ l·ªùi. "
    message = pre_message + message

    # result =  openrouter(message)
    result = deepseek_call(message)
    # result =  openai_scaleway(message)
        
    return result
#X·ª≠ l√Ω l·ªánh /analyze
@bot.message_handler(commands=['analyze'])
def analyze_command(message):
    url = message.text.replace('/analyze', '').strip()
    user_id = message.from_user.id
    group_id = message.chat.id
    user_name = track_id(user_id)
    if user_name == -1:
        #track user id
        response = f"(ID: {user_id})\n\n"
        response += "\n\nƒê√¢y l√† l·∫ßn ƒë·∫ßu ti√™n tao n√≥i chuy·ªán v·ªõi m√†y, m√†y ch·ªù tao c·∫≠p nh·∫≠t c∆° s·ªü d·ªØ li·ªáu v√† coi th·ª≠ m√†y l√† tml n√†o ƒë√£ nh√©!"
        bot.reply_to(message, response)
        return
    if not url:
        bot.reply_to(message, "Nh·∫≠p url sau l·ªánh /analyze th·∫±ng ml.")
        return
    bot.reply_to(message, "ƒêang truy xu·∫•t n·ªôi dung t·ª´ URL...")
    content = extract_content_from_url(url)
    
    if "L·ªói" in content:
        bot.reply_to(message, content)
        return
    
    # Ph√¢n t√≠ch n·ªôi dung b·∫±ng OpenAI
    bot.reply_to(message, "ƒêang ph√¢n t√≠ch n·ªôi dung...")
    analysis = analyze_content_with_openai(content)
    conversation_manager.add_message(group_id, user_id, user_name, "Ph√¢n t√≠ch b√†i b√°o n√†y cho tao", analysis)
    # G·ª≠i k·∫øt qu·∫£ v·ªÅ nh√≥m chat
    bot.reply_to(message, f"**K·∫øt qu·∫£ ph√¢n t√≠ch**:\n{analysis}")
#X·ª≠ l√Ω l·ªánh /ask
@bot.message_handler(commands=['ask'])
def ask_command(message):
    question = message.text.replace('/ask', '').strip()
    user_id = message.from_user.id
    group_id = message.chat.id
    user_name = track_id(user_id)
    if user_name == -1:
        #track user id
        response = f"(ID: {user_id})\n\n"
        response += "\n\nƒê√¢y l√† l·∫ßn ƒë·∫ßu ti√™n tao n√≥i chuy·ªán v·ªõi m√†y, m√†y ch·ªù tao c·∫≠p nh·∫≠t c∆° s·ªü d·ªØ li·ªáu v√† coi th·ª≠ m√†y l√† tml n√†o ƒë√£ nh√©!"
        bot.reply_to(message, response)
        return
    if not question:
        bot.reply_to(message, "Nh·∫≠p c√¢u h·ªèi sau l·ªánh /ask th·∫±ng ml.")
        return
    #G·ª≠i c√¢u h·ªèi qua AI API
    clarify = f"K·∫øt th√∫c ph·∫ßn l·ªãch s·ª≠ tr√≤ chuy·ªán. B√¢y gi·ªù h√£y tr·∫£ l·ªùi c√¢u h·ªèi ƒë·∫øn t·ª´ {user_name}: {question}"
    history = conversation_manager.get_conversation_context(group_id, user_id)
    prompt = history + clarify 
    response = chatbot(prompt)
    conversation_manager.add_message(group_id, user_id, user_name, question, response)
    bot.reply_to(message, response)

def get_chunk(content, chunk_size=4096):
    list_chunk = []
    for i in range(0, len(content), chunk_size):
        list_chunk.append(content[i:i+chunk_size])
    return list_chunk

@bot.message_handler(commands=['domestic-news'])
def handle_news(message):
    group_id = message.chat.id
    # Th√¥ng b√°o ƒëang l·∫•y tin t·ª©c
    processing_msg = bot.reply_to(message, "ƒêang thu th·∫≠p tin t·ª©c t·ª´ c√°c ngu·ªìn...")
    
    # L·∫•y tin t·ª©c t·ª´ RSS
    news_items = fetch_news()
    if not news_items:
        bot.edit_message_text("Kh√¥ng t√¨m th·∫•y tin t·ª©c n√†o!",
                              chat_id=message.chat.id,
                              message_id=processing_msg.message_id)
        return
    
    # T√≥m t·∫Øt tin t·ª©c b·∫±ng OpenAI
    bot.edit_message_text("ƒêang t√≥m t·∫Øt tin t·ª©c...",
                          chat_id=message.chat.id,
                          message_id=processing_msg.message_id)
    summary = summarize_news(news_items)
    conversation_manager.add_message(group_id, '', '', "T√≥m t·∫Øt tin t·ª©c trong n∆∞·ªõc ng√†y h√¥m nay", summary)
    
    # G·ª≠i k·∫øt qu·∫£ t√≥m t·∫Øt
    today = datetime.now().strftime("%d/%m/%Y %H:%M")
    chunk_msg = get_chunk(summary)
    bot.edit_message_text(f"üì∞ T√ìM T·∫ÆT TIN T·ª®C TRONG N∆Ø·ªöC:\n‚è∞ C·∫≠p nh·∫≠t l√∫c: {today}\n\n{chunk_msg[0]}",
                          chat_id=message.chat.id,
                          message_id=processing_msg.message_id)
    if len(chunk_msg) > 1:
        for i in range(1, len(chunk_msg)):
            bot.reply_to(message, chunk_msg[i])
# X·ª≠ l√Ω l·ªánh /search
@bot.message_handler(commands=['search'])
def search_all_sources(message):
    group_id = message.chat.id
    query = message.text.replace('/search', '').strip()
    
    if not query:
        bot.reply_to(message, "Nh·∫≠p ch·ªß ƒë·ªÅ m√†y mu·ªën tao truy xu·∫•t sau l·ªánh /search tml")
        return
    
    bot.reply_to(message, f"ƒêang t√¨m ki·∫øm th√¥ng tin v·ªÅ '{query}' t·ª´ nhi·ªÅu ngu·ªìn. ƒê·ª£i tao t√≠ nha th·∫±ng ml...")
    
    # Thu th·∫≠p th√¥ng tin t·ª´ c√°c ngu·ªìn
    wiki_info = get_wiki_info(query)
    news_info = get_news_info(query, False, count=3)
    reddit_info = get_reddit_info(query, count=3)
    google_info = get_google_search_results(query, num_results=3)

    # T·ªïng h·ª£p t·∫•t c·∫£ th√¥ng tin
    all_info = []
    if isinstance(wiki_info, dict):
        all_info.append(wiki_info)
    else:
        all_info.append({"source": "Wikipedia", "content": wiki_info})
    
    if isinstance(news_info, list):
        all_info.extend(news_info)
    else:
        all_info.append({"source": "News API", "content": news_info})
    
    if isinstance(reddit_info, list):
        all_info.extend(reddit_info)
    else:
        all_info.append({"source": "Reddit", "content": reddit_info})
    if isinstance(google_info, list):
        all_info.extend(google_info)
    else:
        bot.reply_to(message, "t·ª•i m√†y search nhi·ªÅu qu√° d√πng h·∫øt m·∫π API google r·ªìi - donate cho th·∫±ng B·∫£o ƒë·ªÉ n√≥ mua g√≥i vip nh√©")
    # Ph√¢n t√≠ch th√¥ng tin v·ªõi OpenAI
    analysis = analyze_with_openai(query, all_info)
    
    conversation_manager.add_message(group_id, '', '', f"t√¨m ki·∫øm v√† ph√¢n t√≠ch c√°c ngu·ªìn t·ª´ ch·ªß ƒë·ªÅ {query}", analysis)
    # G·ª≠i ph√¢n t√≠ch
    bot.reply_to(message, analysis)

# X·ª≠ l√Ω c√°c l·ªánh ri√™ng l·∫ª cho t·ª´ng ngu·ªìn
@bot.message_handler(commands=['wiki'])
def search_wiki(message):
    query = message.text.replace('/wiki', '').strip()
    if not query:
        bot.reply_to(message, "Vui l√≤ng nh·∫≠p t·ª´ kh√≥a sau l·ªánh /wiki")
        return
    
    bot.reply_to(message, f"ƒêang t√¨m ki·∫øm th√¥ng tin Wikipedia v·ªÅ '{query}'...")
    info = get_wiki_info(query, sentences=10)
    
    if isinstance(info, dict):
        response = f"üìö *{info['title']}*\n\n{info['content']}\n\nNgu·ªìn: {info['url']}"
        response = escape_markdown(response)
    else:
        response = info
    
    bot.reply_to(message, response, parse_mode='Markdown')

@bot.message_handler(commands=['searchimg'])
def search_images(message):
    group_id = message.chat.id 
    query = message.text.replace('/searchimg', '').strip()  # L·∫•y t·ª´ kh√≥a t·ª´ l·ªánh /search
    if not query:
        bot.reply_to(message, "Nh·∫≠p t·ª´ kh√≥a v√†o tml, v√≠ d·ª•: /search m√®o d·ªÖ th∆∞∆°ng")
        return
    
    url = f"https://www.googleapis.com/customsearch/v1?q={query}&key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}&searchType=image&num=5"
    response = requests.get(url)
    data = response.json()
    
    # G·ª≠i t·ª´ng ·∫£nh
    if "items" in data:
        for item in data["items"][:5]:  # Gi·ªõi h·∫°n 5 ·∫£nh
            img_url = item["link"]
            try:
                bot.send_photo(chat_id=message.chat.id, photo=img_url)
            except:
                bot.reply_to(message, "Tao t√¨m ƒë∆∞·ª£c nh∆∞ng ƒë√©o g·ª≠i l√™n ƒë∆∞·ª£c, ch·∫Øc m√†y l·∫°i t√¨m ·∫£nh porn ch·ª© g√¨")

        conversation_manager.add_message(group_id, '', '', f"t√¨m ki·∫øm ·∫£nh v·ªÅ ch·ªß ƒë·ªÅ {query}", "Pussy g·ª≠i tr·∫£ 5 ·∫£nh")
    else:
        bot.reply_to(message, "Kh√¥ng t√¨m th·∫•y ·∫£nh n√†o!")

# ƒêƒÉng k√Ω l·ªánh /search
@bot.message_handler(commands=['news'])
def search_news(message):
    query = message.text.replace('/news', '').strip()
    if not query:
        bot.reply_to(message, "Vui l√≤ng nh·∫≠p t·ª´ kh√≥a sau l·ªánh /news")
        return
    
    bot.reply_to(message, f"ƒêang t√¨m ki·∫øm tin t·ª©c v·ªÅ '{query}'...")
    categories = ["general", "business", "technology", "science", "health", "sports", "entertainment"]
    if query in categories:
        news = get_news_info(query, query)
    else:
        news = get_news_info(query, False)
    if isinstance(news, list):
        for article in news:
            response = f"üì∞ *{article['title']}*\n\n{article['content'][:300]}...\n\nNgu·ªìn: {article['source']}\nNg√†y ƒëƒÉng: {article['published_at']}\nLink: {article['url']}"
            response = escape_markdown(response)
            bot.send_message(message.chat.id, response, parse_mode='MarkdownV2')
    else:
        bot.reply_to(message, news)
"""
@bot.message_handler(commands=['reddit'])
def search_reddit(message):
    query = message.text.replace('/reddit', '').strip()
    if not query:
        bot.reply_to(message, "Vui l√≤ng nh·∫≠p t·ª´ kh√≥a sau l·ªánh /reddit")
        return
    
    bot.reply_to(message, f"ƒêang t√¨m ki·∫øm b√†i vi·∫øt Reddit v·ªÅ '{query}'...")
    posts = get_reddit_info(query)
    
    if isinstance(posts, list):
        for post in posts:
            comment_text = "\n\nüí¨ *B√¨nh lu·∫≠n h√†ng ƒë·∫ßu:*\n"
            for i, comment in enumerate(post['comments']):
                comment_text += f"{i+1}. {comment}\n"
                
            response = f"üîç *{post['title']}*\n\n{post['content'][:300]}...\n\nSubreddit: {post['source']}\nScore: {post['score']}{comment_text}\nNg√†y ƒëƒÉng: {post['created_at']}\nLink: {post['url']}"
            markdown_response = escape_markdown(response)
            bot.send_message(message.chat.id, markdown_response, parse_mode='MarkdownV2')
    else:
        bot.reply_to(message, posts)
"""
def escape_markdown(text):
    # h√†m n√†y ƒë·ªÉ fix l·ªói markdown 
    if text is None:
        return ""
    # Tho√°t c√°c k√Ω t·ª± ƒë·∫∑c bi·ªát
    escape_chars = r'\_*[]()~`>#+-=|{}.!'
    return ''.join(f'\\{c}' if c in escape_chars else c for c in text)
## C·∫•u h√¨nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__) 
## Function ƒë·ªÉ ch·∫°y bot
if __name__ == "__main__":
    logger.info("Bot starting in polling mode")
    bot.remove_webhook()  # X√≥a webhook c≈© n·∫øu c√≥
    bot.polling(none_stop=True, interval=0, timeout=20)
